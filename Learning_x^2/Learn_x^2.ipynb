{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of sets of N random numbers\n",
    "N = 3\n",
    "inputs = torch.randn((10000,N),dtype=torch.float32).cuda()\n",
    "outputs = inputs ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up graph for loss curve\n",
    "fig_loss, ax_loss = plt.subplots()\n",
    "\n",
    "# create a test set\n",
    "test_inputs = torch.randn((1000,N),dtype=torch.float32).cuda()\n",
    "test_outputs = test_inputs ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Create a simple neural network\n",
    "hidden_nodes = 100\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inputs.shape[1],hidden_nodes),\n",
    "    torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(hidden_nodes,hidden_nodes),\n",
    "    # torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_nodes,outputs.shape[1])\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "print('Weights and bias values before training.')\n",
    "for label in model.state_dict().keys():\n",
    "    x = model.state_dict()[label]\n",
    "    # print('\\n{}: '.format(label))\n",
    "    pprint.pprint(x)\n",
    "\n",
    "print('\\nCheck the neural network output before training:')\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test = torch.tensor([0.1, 0.2, 0.3]).cuda()\n",
    "    prediction = model(test)\n",
    "    # print('input: {}, output: {}'.format(test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Get ready to train\n",
    "model.train()\n",
    "\n",
    "# Break the list up into smaller batches for more efficient training\n",
    "numMiniBatch = int(math.floor(inputs.shape[0]/100.))\n",
    "inputMiniBatches = inputs.chunk(numMiniBatch)\n",
    "outputMiniBatches = outputs.chunk(numMiniBatch)\n",
    "\n",
    "# Train the neural network\n",
    "lossFunc = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    # Print something every 10 epochs of training\n",
    "    if epoch%10 == 0:\n",
    "        print('=>Starting {}/{} epochs.'.format(epoch+1,n_epochs))\n",
    "    for minibatch in range(numMiniBatch):\n",
    "        prediction = model(inputMiniBatches[minibatch])\n",
    "        loss = lossFunc(prediction,outputMiniBatches[minibatch])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Plot two points on the loss graph for each epoch, one for testing and one for training.\n",
    "    total_prediction = model(inputs)\n",
    "    total_loss = lossFunc(total_prediction, outputs).item()\n",
    "    test_total_prediction = model(test_inputs)\n",
    "    test_total_loss = lossFunc(test_total_prediction, test_outputs).item()\n",
    "    ax_loss.plot(epoch, total_loss, 'bo', markersize=1, label='Training')\n",
    "    ax_loss.plot(epoch, test_total_loss, 'go', markersize=1, label='Testing')\n",
    "    ax_loss.set_xlabel('Epochs')\n",
    "    ax_loss.set_ylabel('Total Loss')\n",
    "# ax_loss.legend() # I didn't find an easy way to add a legend for this particular graph\n",
    "    \n",
    "\n",
    "\n",
    "print ('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_loss.savefig('loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a single input, not very useful\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test = torch.tensor([0.1, 0.2, 0.3]).cuda()\n",
    "    prediction = model(test)\n",
    "    print('input: {}, prediction: {}'.format(test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the output by running the model in evaluation\n",
    "# make some residual plots\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "fig3, ax3 = plt.subplots()\n",
    "\n",
    "\n",
    "# this can be easily modified to plot output vs. input directly\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for input in test_inputs:\n",
    "        prediction = model(input)\n",
    "        actual = input ** 2\n",
    "        residual = actual - prediction\n",
    "        ax.plot(input[0].item(),residual[0].item(), 'bo', markersize=1)\n",
    "        ax.set_xlabel('1st coordinate input')\n",
    "        ax.set_ylabel('1st coordinate error')\n",
    "        ax2.plot(input[1].item(),residual[1].item(), 'go', markersize=1)\n",
    "        ax2.set_xlabel('2nd coordinate input')\n",
    "        ax2.set_ylabel('2nd coordinate error')\n",
    "        ax3.plot(input[2].item(),residual[2].item(), 'co', markersize=1)\n",
    "        ax3.set_xlabel('3rd coordinate input')\n",
    "        ax3.set_ylabel('3rd coordinate error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('1st-1st.pdf')\n",
    "# fig2.savefig('2nd-2nd.pdf')\n",
    "# fig3.savefig('3rd-3rd.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what proportion of the predictions falls within 0.01 absolute accuracy or 1% relative accuracy\n",
    "def test_data(prediction, actual):\n",
    "    \"Output 1 if prediction is accurate enough and 0 otherwise\"\n",
    "    abs_err = abs(prediction-actual)\n",
    "    rel_err = abs_err / actual\n",
    "    if (abs_err < 0.01 or rel_err < 0.01):\n",
    "        return 1\n",
    "    else: return 0\n",
    "\n",
    "v_test_data = np.vectorize(test_data) # This makes a vector function and avoids a for loop to improve performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't find a way to get around having to .cpu() first before converting to a numpy array\n",
    "accuracy = v_test_data(test_total_prediction.cpu().detach().numpy(), test_outputs.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I realized that this actually treats the coordinates of the vectors as separate data points, so it's\n",
    "# actually computing the proportion of accurate coordinates. To fix this I'll need to find the norm of the vectors\n",
    "# and check accuracy on those. \n",
    "np.sum(accuracy) / torch.numel(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT TESTED\n",
    "# Let's look at the weights and biases\n",
    "# Check the math by hand\n",
    "A = model.state_dict()['0.weight'].tolist()\n",
    "a = model.state_dict()['0.bias'].tolist()\n",
    "print('\\nWeights and biases for first layer.')\n",
    "pprint.pprint(A)\n",
    "print()\n",
    "pprint.pprint(a)\n",
    "\n",
    "B = model.state_dict()['2.weight'].tolist()\n",
    "b = model.state_dict()['2.bias'].tolist()\n",
    "print('\\nWeights and biases for second layer.')\n",
    "pprint.pprint(B)\n",
    "print()\n",
    "pprint.pprint(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out the details\n",
    "x0 = test[0].item()\n",
    "x1 = test[1].item()\n",
    "x2 = test[2].item()\n",
    "\n",
    "print('Input to neural network:')\n",
    "print('x0={:7.4f}, x1={:7.4f},x2={:7.4f}'.format(x0,x1,x2))\n",
    "\n",
    "# Check the math by hand\n",
    "A = model.state_dict()['0.weight'].tolist()\n",
    "a = model.state_dict()['0.bias'].tolist()\n",
    "\n",
    "sum0 = A[0][0]*x0+A[0][1]*x1+A[0][2]*x2+a[0]\n",
    "sum1 = A[1][0]*x0+A[1][1]*x1+A[1][2]*x2+a[1]\n",
    "sum2 = A[2][0]*x0+A[2][1]*x1+A[2][2]*x2+a[2]\n",
    "print('\\nSums for first layer:')\n",
    "print('{:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(sum0,A[0][0],x0,A[0][1],x1,A[0][2],x2,a[0]))\n",
    "print('{:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(sum1,A[1][0],x0,A[1][1],x1,A[1][2],x2,a[1]))\n",
    "print('{:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(sum2,A[2][0],x0,A[2][1],x1,A[2][2],x2,a[2]))\n",
    "\n",
    "y0 = max(sum0,0)\n",
    "y1 = max(sum1,0)\n",
    "y2 = max(sum2,0)\n",
    "print('\\nApplying the ReLU to the sums:')\n",
    "print('y0 = {:7.4f} = ReLU({:7.4f})'.format(y0,sum0))\n",
    "print('y1 = {:7.4f} = ReLU({:7.4f})'.format(y1,sum1))\n",
    "print('y2 = {:7.4f} = ReLU({:7.4f})'.format(y2,sum2))\n",
    "\n",
    "B = model.state_dict()['2.weight'].tolist()\n",
    "b = model.state_dict()['2.bias'].tolist()\n",
    "\n",
    "z0 = B[0][0]*y0+B[0][1]*y1+B[0][2]*y2+b[0]\n",
    "z1 = B[1][0]*y0+B[1][1]*y1+B[1][2]*y2+b[1]\n",
    "z2 = B[2][0]*y0+B[2][1]*y1+B[2][2]*y2+b[2]\n",
    "print('\\nCalculating the final output:')\n",
    "print('z0 = {:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(z0,B[0][0],y0,B[0][1],y1,B[0][2],y2,b[0]))\n",
    "print('z1 = {:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(z1,B[1][0],y0,B[1][1],y1,B[1][2],y2,b[1]))\n",
    "print('z2 = {:7.4f} = {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f} * {:7.4f} + {:7.4f}'.format(z2,B[2][0],y0,B[2][1],y1,B[2][2],y2,b[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
