{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42ad605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_Module_Poly as nnm\n",
    "import torch\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import argparse\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9cd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# DEPRECATED\n",
    "def evaluate_legacy(inputs):\n",
    "    # Assumes model uses square and cross terms as well as standardization\n",
    "    # Assumes model takes in 152 terms\n",
    "    # Currently only uses a slice of inputs_all\n",
    "    all_shape = list(inputs.shape) # The shape of the final input tensor, including the square and cross terms\n",
    "    all_shape[-1] = 152\n",
    "    cross_shape = list(inputs.shape) # The shape of the input cross terms tensor\n",
    "    cross_shape[-1] = 120\n",
    "    inputs_all = torch.zeros(all_shape).cuda()\n",
    "    squares = inputs**2\n",
    "    cross_terms = torch.zeros(cross_shape).cuda()\n",
    "    idx = 0\n",
    "    for i in range(16):\n",
    "        for j in range(i):\n",
    "            cross_terms[...,idx] = inputs[...,i] * inputs[...,j]\n",
    "            idx += 1\n",
    "    inputs_all[...,0:16] = inputs\n",
    "    inputs_all[...,16:32] = squares\n",
    "    inputs_all[...,32:152] = cross_terms\n",
    "    std_inputs = nnm.affine_transform(inputs_all, input_stats)\n",
    "    std_outputs = model(std_inputs)\n",
    "    outputs = nnm.affine_untransform(std_outputs, output_stats)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def evaluate(inputs, model, input_stats, output_stats):\n",
    "    # Assumes model was trained using square and cross terms as well as standardization\n",
    "    # Assumes model takes in 16 terms and an internal layer calculates the square and cross terms\n",
    "    # Currently only uses a slice of inputs_all\n",
    "    std_inputs = nnm.affine_transform(inputs, input_stats)\n",
    "    std_outputs = model(std_inputs)\n",
    "    outputs = nnm.affine_untransform(std_outputs, output_stats)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36cfd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = '6755_13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae6bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {}\n",
    "names['cQei'] = 0\n",
    "names['cQl3i'] = 1\n",
    "names['cQlMi'] = 2\n",
    "names['cbW'] = 3\n",
    "names['cpQ3'] = 4\n",
    "names['cpQM'] = 5\n",
    "names['cpt'] = 6\n",
    "names['cptb'] = 7\n",
    "names['ctG'] = 8\n",
    "names['ctW'] = 9\n",
    "names['ctZ'] = 10\n",
    "names['ctei'] = 11\n",
    "names['ctlSi'] = 12\n",
    "names['ctlTi'] = 13\n",
    "names['ctli'] = 14\n",
    "names['ctp'] = 15\n",
    "\n",
    "WC_to_analyze_1 = ['cpQ3', 'cpQM', 'cpt', 'cptb', 'cQei', 'ctei', 'ctli', 'ctW'] # The first of the two WCs to graph\n",
    "WC_to_analyze_2 = ['cbW', 'ctG', 'ctp', 'cQl3i', 'cQlMi', 'ctlTi', 'ctlSi', 'ctZ'] # The second of the two WCs to graph\n",
    "\n",
    "batch_size = 4096\n",
    "epochs = 100\n",
    "random_starting_points = 50 # The number of random starting points to do gradient descent on for each scanned value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6e0fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = torch.load(f'./{out_file}_model+.pt')\n",
    "best_model_state = save_dict['model']\n",
    "parameters_save = save_dict['parameters']\n",
    "input_stats = save_dict['input_stats']\n",
    "output_stats = save_dict['output_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbb22e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure we're on cuda\n",
    "input_stats[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0523d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00497152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated() / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c4e052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N': 16,\n",
       " 'train_size': 49378587,\n",
       " 'test_size': 498773,\n",
       " 'hidden_nodes': 750,\n",
       " 'hidden_layers': 2,\n",
       " 'batch_size': 1024,\n",
       " 'n_epochs': 152,\n",
       " 'learning_rate': 0.0001,\n",
       " 'lr_red_factor': 0.2,\n",
       " 'lr_red_patience': 20.0,\n",
       " 'lr_red_threshold': 1e-06,\n",
       " 'weight_decay': 0.0,\n",
       " 'accu_out_resolution': 100,\n",
       " 'out_residual_resolution': 100}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3badde11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): PolynomialLayer(\n",
       "    (monomial_layer): MonomialLayer()\n",
       "    (linear): Linear(in_features=152, out_features=750, bias=True)\n",
       "  )\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=750, out_features=750, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=750, out_features=750, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=750, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nnm.create_model(parameters_save['N'], 1, parameters_save)\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd0d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_frozen_2D_data = {}\n",
    "actual_profiled_2D_data = {}\n",
    "model_frozen_2D_data = {}\n",
    "model_profiled_2D_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ba4e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just test the first graph\n",
    "nums = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc9e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.06825228\n",
      "-0.99903536\n",
      "-0.09825421\n",
      "-0.00508612\n",
      "-0.00581771\n",
      "-0.006350204\n",
      "-0.0060666674\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# actual frozen data\n",
    "for num in nums:\n",
    "    WC1 = WC_to_analyze_1[num]\n",
    "    WC2 = WC_to_analyze_2[num]\n",
    "    loaded = np.load(f'likelihood_{WC1}_{WC2}.npz')\n",
    "    actual_frozen_2D_data[str(num)] = {WC1: loaded[WC1], WC2: loaded[WC2], 'deltaNLL': loaded['deltaNLL']}\n",
    "    print (actual_frozen_2D_data[str(num)]['deltaNLL'].min())\n",
    "    actual_frozen_2D_data[str(num)]['deltaNLL'] -= actual_frozen_2D_data[str(num)]['deltaNLL'].min()\n",
    "    actual_frozen_2D_data[str(num)]['deltaNLL'] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e2b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual profiled data\n",
    "for num in nums:\n",
    "    WC1 = WC_to_analyze_1[num]\n",
    "    WC2 = WC_to_analyze_2[num]\n",
    "    loaded = np.load(f'likelihood_profiled_{WC1}_{WC2}.npz')\n",
    "    inputs = np.zeros((loaded['deltaNLL'].shape[0], random_starting_points, 16))\n",
    "    for key2 in names.keys():\n",
    "        inputs[...,names[key2]] = loaded[key2][:,np.newaxis] # broadcast into the new axis of random starting points\n",
    "    actual_profiled_2D_data[str(num)] = {'inputs': inputs, 'deltaNLL': loaded['deltaNLL']}\n",
    "    actual_profiled_2D_data[str(num)]['deltaNLL'] -= actual_profiled_2D_data[str(num)]['deltaNLL'].min()\n",
    "    actual_profiled_2D_data[str(num)]['deltaNLL'] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b435de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4416199\n",
      "1.6867123\n",
      "1.385086\n",
      "3.622654\n",
      "3.6906662\n",
      "3.7166958\n",
      "3.5859928\n",
      "2.0241241\n"
     ]
    }
   ],
   "source": [
    "# model frozen data\n",
    "for num in nums:\n",
    "    inputs_y = actual_frozen_2D_data[str(num)][WC_to_analyze_1[num]]\n",
    "    inputs_x = actual_frozen_2D_data[str(num)][WC_to_analyze_2[num]]\n",
    "    num_inputs = inputs_y.shape[0]\n",
    "    inputs = np.zeros((num_inputs, 16))\n",
    "    inputs[:,names[WC_to_analyze_1[num]]] = inputs_y\n",
    "    inputs[:,names[WC_to_analyze_2[num]]] = inputs_x\n",
    "\n",
    "    std_inputs = nnm.affine_transform(torch.from_numpy(inputs).float().cuda(), input_stats)\n",
    "    std_outputs = torch.full((num_inputs, 1), 100.).cuda() # fill outputs with 100. as a default (decimal to force dtype=float not int)\n",
    "    inputMiniBatches = torch.split(std_inputs, batch_size)\n",
    "    batch_idx = 0\n",
    "    for minibatch in range(len(inputMiniBatches)):\n",
    "        batch_outputs = model(inputMiniBatches[minibatch])\n",
    "        std_outputs[batch_idx: batch_idx + batch_outputs.shape[0]] = batch_outputs\n",
    "        batch_idx += batch_outputs.shape[0]\n",
    "    outputs = nnm.affine_untransform(std_outputs, output_stats).cpu().detach().numpy().flatten()\n",
    "    print (outputs.min())\n",
    "    outputs -= outputs.min()\n",
    "    outputs *= 2\n",
    "    model_frozen_2D_data[str(num)] = {WC_to_analyze_1[num]: inputs[:,names[WC_to_analyze_1[num]]], WC_to_analyze_2[num]: inputs[:,names[WC_to_analyze_2[num]]], 'deltaNLL': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3c9e4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.18279624 -9.96666622 -9.89999962 ...  9.83333302  9.89999962\n",
      "  9.96666622]\n",
      "Starting 0/22 minibatches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in ReluBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 668, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 456, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 445, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 352, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 647, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 335, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/srv/ipykernel_62/2853164373.py\", line 40, in <module>\n",
      "    batch_outputs = evaluate(batch_inputs, model, input_stats, output_stats)\n",
      "  File \"/srv/ipykernel_62/837671589.py\", line 33, in evaluate\n",
      "    std_outputs = model(std_inputs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py\", line 98, in forward\n",
      "    return F.relu(input, inplace=self.inplace)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 1442, in relu\n",
      "    result = torch.relu(input)\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 586.00 MiB (GPU 0; 22.17 GiB total capacity; 6.64 GiB already allocated; 159.94 MiB free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/srv/ipykernel_62/2853164373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mmin_WCs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_to_update\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_to_update\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mbatch_outputs_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m#print (torch.count_nonzero(inputs.grad))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m#print (inputs.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 586.00 MiB (GPU 0; 22.17 GiB total capacity; 6.64 GiB already allocated; 159.94 MiB free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model profiled data\n",
    "for num in nums:\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    inputs_old = actual_profiled_2D_data[str(num)]['inputs']\n",
    "    num_inputs = inputs_old.shape[0]\n",
    "    model_profiled_2D_data[str(num)] = {WC_to_analyze_1[num]: inputs_old[..., 0, names[WC_to_analyze_1[num]]], WC_to_analyze_2[num]: inputs_old[..., 0, names[WC_to_analyze_2[num]]]}\n",
    "    print (model_profiled_2D_data[str(num)][WC_to_analyze_2[num]])\n",
    "    inputs = (np.random.random_sample(inputs_old.shape) - 0.5) * 40\n",
    "    inputs[...,names[WC_to_analyze_1[num]]] = inputs_old[...,names[WC_to_analyze_1[num]]] # copy over the WC being scanned, while leaving the other 15 randomized\n",
    "    inputs[...,names[WC_to_analyze_2[num]]] = inputs_old[...,names[WC_to_analyze_2[num]]] # copy over the WC being scanned, while leaving the other 15 randomized\n",
    "    inputs = torch.from_numpy(inputs).float().cuda()\n",
    "    inputs.requires_grad = True\n",
    "\n",
    "    outputs = torch.full((num_inputs, 1), 100.).cuda() # fill outputs with 100. as a default (decimal to force dtype=float not int)\n",
    "    inputMiniBatches = torch.split(inputs, batch_size)\n",
    "    batch_idx = 0\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Memory debugging info\n",
    "    memory_fig, memory_ax = plt.subplots()\n",
    "    memory_count = 0\n",
    "    memory_count_arr = np.full((len(inputMiniBatches)*epochs), np.NaN)\n",
    "    memory_vals = np.full((len(inputMiniBatches)*epochs), np.NaN)\n",
    "\n",
    "    for minibatch in range(len(inputMiniBatches)):\n",
    "        optimizer = torch.optim.Adam([inputs],lr=1e-0)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=5, threshold=1e-6)\n",
    "        inputMiniBatches = torch.split(inputs, batch_size)\n",
    "        batch_inputs = inputMiniBatches[minibatch]\n",
    "        min_outputs = evaluate(batch_inputs, model, input_stats, output_stats) # The outputs of the random starting points, to be updated every epoch\n",
    "        min_WCs = batch_inputs.detach().clone() # A snapshot of the WCs of all the points to scan\n",
    "        optimizer.zero_grad()\n",
    "        print (f'Starting {minibatch}/{len(inputMiniBatches)} minibatches.')\n",
    "        for epoch in range(epochs):\n",
    "            #print (f'Starting {epoch}/{epochs} epochs of {minibatch}/{len(inputMiniBatches)} minibatches.')\n",
    "\n",
    "            inputMiniBatches = torch.split(inputs, batch_size)\n",
    "            batch_inputs = inputMiniBatches[minibatch]\n",
    "            batch_outputs = evaluate(batch_inputs, model, input_stats, output_stats)\n",
    "            batch_outputs_cp = batch_outputs.detach().clone()\n",
    "            batch_outputs_sum = torch.sum(torch.log(batch_outputs + 10)) # Optimize the sum of outputs\n",
    "            idx_to_update = torch.where(batch_outputs_cp < min_outputs)[0]\n",
    "            min_outputs[idx_to_update] = batch_outputs_cp[idx_to_update]\n",
    "            min_WCs[idx_to_update] = batch_inputs.detach().clone()[idx_to_update]\n",
    "            optimizer.zero_grad()\n",
    "            batch_outputs_sum.backward()\n",
    "            #print (torch.count_nonzero(inputs.grad))\n",
    "            #print (inputs.grad)\n",
    "            #print (inputs.grad[1500,31,6])\n",
    "            #print (f'Sum of minibatch outputs: {batch_outputs_sum}')\n",
    "            #print (f'Output of a random point: {batch_outputs[500,0,0]}')\n",
    "            inputs.grad[...,names[WC_to_analyze_1[num]]] = 0\n",
    "            inputs.grad[...,names[WC_to_analyze_2[num]]] = 0\n",
    "            optimizer.step()\n",
    "            #scheduler.step(output)\n",
    "\n",
    "            # Memory debugging info\n",
    "            memory_count_arr[memory_count] = memory_count\n",
    "            memory_vals[memory_count] = torch.cuda.memory_allocated()/1e9\n",
    "            memory_ax.cla()\n",
    "            memory_ax.plot(memory_count_arr, memory_vals)\n",
    "            memory_count += 1\n",
    "\n",
    "        (min_outputs_scanned, min_starting_point_indicies) = torch.min(min_outputs, -2) # Get the best all starting points\n",
    "\n",
    "        # Make the index tensor suitable for gathering the min_WCs\n",
    "        min_starting_point_indicies = min_starting_point_indicies.unsqueeze(-2)\n",
    "        min_starting_point_indicies_shape = list(min_starting_point_indicies.shape)\n",
    "        min_starting_point_indicies_shape[-1] = 16\n",
    "        min_starting_point_indicies = min_starting_point_indicies.expand(min_starting_point_indicies_shape) # Warning: don't perform in-place operations on this since expand() does not allocate new memory\n",
    "\n",
    "        min_WCs_scanned = torch.gather(min_WCs, -2, min_starting_point_indicies) # Get the WCs corresponding to the best-performing starting points\n",
    "        outputs[batch_idx: batch_idx + batch_outputs.shape[0]] = min_outputs_scanned.detach().clone() # detach from graph to delete obsolete graphs from memory! This was the culprit causing the memory leak\n",
    "        print (WC_to_analyze_1[num] + ' ' + WC_to_analyze_2[num] + ' ' + 'batch number: ' + str(minibatch))\n",
    "        print (min_WCs_scanned)\n",
    "        batch_idx += batch_outputs.shape[0]\n",
    "    print ('Profiling done!')\n",
    "    print (f'Time used: {time.perf_counter() - start_time} seconds.')\n",
    "    outputs = outputs.cpu().detach().numpy().flatten()\n",
    "    outputs -= outputs.min()\n",
    "    outputs *= 2\n",
    "    model_profiled_2D_data[str(num)]['deltaNLL'] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bbafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff data\n",
    "frozen_diff_data = copy.deepcopy(actual_frozen_2D_data)\n",
    "profiled_diff_data = copy.deepcopy(model_profiled_2D_data) # since actual_profiled_2D_data actually has all 16 variables stored in it\n",
    "for num in nums:\n",
    "    frozen_diff_data[str(num)]['deltaNLL'] = model_frozen_2D_data[str(num)]['deltaNLL'] - actual_frozen_2D_data[str(num)]['deltaNLL']\n",
    "    profiled_diff_data[str(num)]['deltaNLL'] = model_profiled_2D_data[str(num)]['deltaNLL'] - actual_profiled_2D_data[str(num)]['deltaNLL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf5c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozen contours\n",
    "frozen_contours = {}\n",
    "for num in nums:\n",
    "    frozen_contours[str(num)] = plt.subplots()\n",
    "    actual_contour = frozen_contours[str(num)][1].tricontour(actual_frozen_2D_data[str(num)][WC_to_analyze_2[num]], actual_frozen_2D_data[str(num)][WC_to_analyze_1[num]], actual_frozen_2D_data[str(num)]['deltaNLL'], colors='k', linestyles=['dashed', 'dashdot', 'dotted'], levels=[2.30, 6.18, 11.83]) # 1, 2, and 3 sigmas\n",
    "    model_contour = frozen_contours[str(num)][1].tricontour(model_frozen_2D_data[str(num)][WC_to_analyze_2[num]], model_frozen_2D_data[str(num)][WC_to_analyze_1[num]], model_frozen_2D_data[str(num)]['deltaNLL'], colors='r', linestyles=['dashed', 'dashdot', 'dotted'], levels=[2.30, 6.18, 11.83]) # 1, 2, and 3 sigmas\n",
    "    SM_value = frozen_contours[str(num)][1].scatter(0, 0, marker='d', c='gold', ec='royalblue', s=30, linewidths=1, zorder=10)\n",
    "    frozen_contours[str(num)][1].legend(actual_contour.collections+model_contour.collections+[SM_value], ['$1\\sigma$ target', '$2\\sigma$ target', '$3\\sigma$ target', '$1\\sigma$ predicted', '$2\\sigma$ predicted', '$3\\sigma$ predicted', 'SM value'])\n",
    "    frozen_contours[str(num)][1].set_xlabel(WC_to_analyze_2[num])\n",
    "    frozen_contours[str(num)][1].set_ylabel(WC_to_analyze_1[num])\n",
    "    frozen_contours[str(num)][1].set_title('Frozen')\n",
    "    frozen_contours[str(num)][0].tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiled contours\n",
    "profiled_contours = {}\n",
    "for num in nums:\n",
    "    profiled_contours[str(num)] = plt.subplots()\n",
    "    actual_contour = profiled_contours[str(num)][1].tricontour(model_profiled_2D_data[str(num)][WC_to_analyze_2[num]], model_profiled_2D_data[str(num)][WC_to_analyze_1[num]], actual_profiled_2D_data[str(num)]['deltaNLL'], colors='k', linestyles=['dashed', 'dashdot', 'dotted'], levels=[2.30, 6.18, 11.83]) # 1, 2, and 3 sigmas\n",
    "    model_contour = profiled_contours[str(num)][1].tricontour(model_profiled_2D_data[str(num)][WC_to_analyze_2[num]], model_profiled_2D_data[str(num)][WC_to_analyze_1[num]], model_profiled_2D_data[str(num)]['deltaNLL'], colors='r', linestyles=['dashed', 'dashdot', 'dotted'], levels=[2.30, 6.18, 11.83]) # 1, 2, and 3 sigmas\n",
    "    SM_value = profiled_contours[str(num)][1].scatter(0, 0, marker='d', c='gold', ec='royalblue', s=30, linewidths=1, zorder=10)\n",
    "    profiled_contours[str(num)][1].legend(actual_contour.collections+model_contour.collections+[SM_value], ['$1\\sigma$ target', '$2\\sigma$ target', '$3\\sigma$ target', '$1\\sigma$ predicted', '$2\\sigma$ predicted', '$3\\sigma$ predicted', 'SM value'])\n",
    "    profiled_contours[str(num)][1].set_xlabel(WC_to_analyze_2[num])\n",
    "    profiled_contours[str(num)][1].set_ylabel(WC_to_analyze_1[num])\n",
    "    profiled_contours[str(num)][1].set_title('Profiled')\n",
    "    profiled_contours[str(num)][0].tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea134742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff graphs\n",
    "frozen_diff = {}\n",
    "profiled_diff = {}\n",
    "for num in nums:\n",
    "    WC2 = frozen_diff_data[str(num)][WC_to_analyze_2[num]]\n",
    "    WC1 = frozen_diff_data[str(num)][WC_to_analyze_1[num]]\n",
    "    deltaNLL = frozen_diff_data[str(num)]['deltaNLL']\n",
    "    \n",
    "    frozen_diff[str(num)] = plt.subplots()\n",
    "    im = frozen_diff[str(num)][1].tripcolor(WC2, WC1, deltaNLL)\n",
    "    frozen_diff[str(num)][1].set_xlabel(WC_to_analyze_2[num])\n",
    "    frozen_diff[str(num)][1].set_ylabel(WC_to_analyze_1[num])\n",
    "    frozen_diff[str(num)][1].set_title('Frozen')\n",
    "    frozen_diff[str(num)][0].colorbar(im, ax=frozen_diff[str(num)][1], label='prediction - target')\n",
    "    frozen_diff[str(num)][0].tight_layout()\n",
    "\n",
    "for num in nums:\n",
    "    WC2 = profiled_diff_data[str(num)][WC_to_analyze_2[num]]\n",
    "    WC1 = profiled_diff_data[str(num)][WC_to_analyze_1[num]]\n",
    "    deltaNLL = profiled_diff_data[str(num)]['deltaNLL']\n",
    "    \n",
    "    profiled_diff[str(num)] = plt.subplots()\n",
    "    im = profiled_diff[str(num)][1].tripcolor(WC2, WC1, deltaNLL)\n",
    "    profiled_diff[str(num)][1].set_xlabel(WC_to_analyze_2[num])\n",
    "    profiled_diff[str(num)][1].set_ylabel(WC_to_analyze_1[num])\n",
    "    profiled_diff[str(num)][1].set_title('Profiled')\n",
    "    profiled_diff[str(num)][0].colorbar(im, ax=profiled_diff[str(num)][1], label='prediction - target')\n",
    "    profiled_diff[str(num)][0].tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_profiled_2D_data[str(num)]['deltaNLL'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94952e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_profiled_2D_data[str(num)][WC_to_analyze_1[num]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_profiled_2D_data[str(num)][WC_to_analyze_2[num]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeafa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save graphs to pdf\n",
    "pp = PdfPages(f'{out_file}_validation_2D_with_profiling.pdf')\n",
    "for key in frozen_contours.keys():\n",
    "    pp.savefig(frozen_contours[key][0])\n",
    "for key in profiled_contours.keys():\n",
    "    pp.savefig(profiled_contours[key][0])\n",
    "for key in frozen_diff.keys():\n",
    "    pp.savefig(frozen_diff[key][0])\n",
    "for key in profiled_diff.keys():\n",
    "    pp.savefig(profiled_diff[key][0])\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baeb3e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.033333335\n",
      "2.1214182\n",
      "2.3297958\n"
     ]
    }
   ],
   "source": [
    "# Check individual points\n",
    "idx = 50267\n",
    "num = 0\n",
    "# print (model_profiled_2D_data[str(num)]['cpQ3'][idx])\n",
    "# print (model_profiled_2D_data[str(num)]['cbW'][idx])\n",
    "# print (model_profiled_2D_data[str(num)]['deltaNLL'][idx])\n",
    "print (actual_frozen_2D_data[str(num)]['cpQ3'][idx])\n",
    "print (actual_frozen_2D_data[str(num)]['cbW'][idx])\n",
    "print (actual_frozen_2D_data[str(num)]['deltaNLL'][idx])\n",
    "print (model_frozen_2D_data[str(num)]['deltaNLL'][idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
